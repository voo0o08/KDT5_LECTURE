{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer 냅다 공부하는 파일\n",
    "## 프로젝트 주제 : 성경 번역 프로그램 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 테이블의 데이터수는 총 31102개 입니다.\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "# 데이터 베이스 연결하기\n",
    "conn = pymysql.connect(host='1.251.203.204',\n",
    "                       user='root',\n",
    "                       password='kdt5',\n",
    "                       db='Team4',\n",
    "                       charset='utf8',\n",
    "                       port=33065)\n",
    "\n",
    "curs = conn.cursor()\n",
    "\n",
    "# 검색 명령어 사용 \n",
    "sql = \"SELECT eng.text as eng_text, kor.text as kor_text FROM language_en eng join language_ko kor on eng.id = kor.id;\"\n",
    "curs.execute(sql)\n",
    "result = curs.fetchall()\n",
    "print(\"현재 테이블의 데이터수는 총 {}개 입니다.\".format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL-PC\\AppData\\Local\\Temp\\ipykernel_3952\\1632777522.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_sql(sql, conn)\n",
    "\n",
    "# 데이터베이스 연결 종료\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_text</th>\n",
       "      <th>kor_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the beginning God created the heaven and th...</td>\n",
       "      <td>태초에 하나님이 천지를 창조하시니라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And the earth was without form, and void; and ...</td>\n",
       "      <td>땅이 혼돈하고 공허하며 흑암이 깊음 위에 있고 하나님의 신은 수면에 운행하시니라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>하나님이 가라사대 빛이 있으라 하시매 빛이 있었고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>그 빛이 하나님의 보시기에 좋았더라 하나님이 빛과 어두움을 나누사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>빛을 낮이라 칭하시고 어두움을 밤이라 칭하시니라 저녁이 되며 아침이 되니 이는 첫째...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            eng_text  \\\n",
       "0  In the beginning God created the heaven and th...   \n",
       "1  And the earth was without form, and void; and ...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God saw the light, that it was good: and G...   \n",
       "4  And God called the light Day, and the darkness...   \n",
       "\n",
       "                                            kor_text  \n",
       "0                                태초에 하나님이 천지를 창조하시니라  \n",
       "1       땅이 혼돈하고 공허하며 흑암이 깊음 위에 있고 하나님의 신은 수면에 운행하시니라  \n",
       "2                        하나님이 가라사대 빛이 있으라 하시매 빛이 있었고  \n",
       "3               그 빛이 하나님의 보시기에 좋았더라 하나님이 빛과 어두움을 나누사  \n",
       "4  빛을 낮이라 칭하시고 어두움을 밤이라 칭하시니라 저녁이 되며 아침이 되니 이는 첫째...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿼리 -> dataframe결과 출력\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 프레임 iter로 만들기(내가 쓰는 version은 TabularDataset지원 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_iterator(df):\n",
    "    for i, row in df.iterrows():\n",
    "        yield row[\"eng_text\"], row[\"kor_text\"] \n",
    "\n",
    "split_idx = int(len(df) * 0.8)  # 전체 데이터 중 80%를 훈련 데이터셋으로 사용\n",
    "train_df = df.iloc[:split_idx]\n",
    "valid_df = df.iloc[split_idx:]\n",
    "\n",
    "# train_iter = dataframe_iterator(train_df)\n",
    "# valid_iter = dataframe_iterator(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        In the beginning God created the heaven and th...\n",
       "1        And the earth was without form, and void; and ...\n",
       "2        And God said, Let there be light: and there wa...\n",
       "3        And God saw the light, that it was good: and G...\n",
       "4        And God called the light Day, and the darkness...\n",
       "                               ...                        \n",
       "24876    And they said among themselves, Who shall roll...\n",
       "24877    And when they looked, they saw that the stone ...\n",
       "24878    And entering into the sepulchre, they saw a yo...\n",
       "24879    And he saith unto them, Be not affrighted: Ye ...\n",
       "24880    But go your way, tell his disciples and Peter ...\n",
       "Name: eng_text, Length: 24881, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"eng_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24881, 6221)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"eng_text\"].shape[0], valid_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class CustomImageDataset(Dataset):\n",
    "#     def __init__(self, source=None, target=None):\n",
    "#         self.source = source\n",
    "#         self.target = target\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.source.shape[0]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.source[idx], self.target[idx]\n",
    "\n",
    "# train_iter = CustomImageDataset(train_df[\"eng_text\"], train_df[\"kor_text\"])\n",
    "# valid_iter = CustomImageDataset(valid_df[\"eng_text\"], valid_df[\"kor_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL-PC\\.conda\\envs\\EXAM_DL\\lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Transform:\n",
      "{'eng': functools.partial(<function _spacy_tokenize at 0x000001C387BB1DC0>, spacy=<spacy.lang.en.English object at 0x000001C387E9F940>), 'kor': <bound method Okt.morphs of <konlpy.tag._okt.Okt object at 0x000001C3E8E59220>>}\n",
      "Vocab Transform:\n",
      "{'eng': Vocab(), 'kor': Vocab()}\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "\n",
    "def generate_tokens(text_iter, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for text in text_iter:\n",
    "        yield token_transform[language](text[language_index[language]])\n",
    "\n",
    "SRC_LANGUAGE = \"eng\"\n",
    "TGT_LANGUAGE = \"kor\"\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # 알 수 없음 / 패딩 / 인덱스 시작 / 인덱스 끝 \n",
    "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "token_transform = {\n",
    "    SRC_LANGUAGE: get_tokenizer(\"spacy\", language=\"en\"),\n",
    "    TGT_LANGUAGE: get_tokenizer(Okt().morphs),\n",
    "}\n",
    "print(\"Token Transform:\")\n",
    "print(token_transform)\n",
    "\n",
    "vocab_transform = {} # 딕셔너리 {\"eng\":어쩌구, \"kor\":저쩌구}\n",
    "train_iter = train_df.values # 현길사마가 도와준 부분 \n",
    "val_iter = valid_df.values\n",
    "\n",
    "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[language] = build_vocab_from_iterator(\n",
    "        generate_tokens(train_iter, language),\n",
    "        min_freq=1, # 토큰화된 단어들의 최소 빈도수 지정 \n",
    "        specials=special_symbols, # 트랜스포머에 사용하는 특수토큰 [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "        special_first=True, # 특수 토큰을 단어 집합의 맨 앞에 추가 \n",
    "    )\n",
    "\n",
    "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[language].set_default_index(UNK_IDX)\n",
    "\n",
    "print(\"Vocab Transform:\")\n",
    "print(vocab_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# 포지셔널 인코딩 \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# TokenEmbedding 클래스로 소스 데이터와 입력 데이터를 입력 임베딩으로 변환\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        emb_size,\n",
    "        max_len,\n",
    "        nhead,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        dim_feedforward,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            d_model=emb_size, max_len=max_len, dropout=dropout\n",
    "        )\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        trg,\n",
    "        src_mask,\n",
    "        tgt_mask,\n",
    "        src_padding_mask,\n",
    "        tgt_padding_mask,\n",
    "        memory_key_padding_mask,\n",
    "    ):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=None,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.transformer.encoder(\n",
    "            self.positional_encoding(self.src_tok_emb(src)), src_mask\n",
    "        )\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        return self.transformer.decoder(\n",
    "            self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL-PC\\.conda\\envs\\EXAM_DL\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "Seq2SeqTransformer                                                     --\n",
       "├─TokenEmbedding: 1-1                                                  --\n",
       "│    └─Embedding: 2-1                                                  1,515,904\n",
       "├─TokenEmbedding: 1-2                                                  --\n",
       "│    └─Embedding: 2-2                                                  3,174,912\n",
       "├─PositionalEncoding: 1-3                                              --\n",
       "│    └─Dropout: 2-3                                                    --\n",
       "├─Transformer: 1-4                                                     --\n",
       "│    └─TransformerEncoder: 2-4                                         --\n",
       "│    │    └─ModuleList: 3-1                                            594,816\n",
       "│    │    └─LayerNorm: 3-2                                             256\n",
       "│    └─TransformerDecoder: 2-5                                         --\n",
       "│    │    └─ModuleList: 3-3                                            793,728\n",
       "│    │    └─LayerNorm: 3-4                                             256\n",
       "├─Linear: 1-5                                                          3,199,716\n",
       "===============================================================================================\n",
       "Total params: 9,279,588\n",
       "Trainable params: 9,279,588\n",
       "Non-trainable params: 0\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torchinfo import summary\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=3,\n",
    "    emb_size=128,\n",
    "    max_len=128,\n",
    "    nhead=8,\n",
    "    src_vocab_size=len(vocab_transform[SRC_LANGUAGE]),\n",
    "    tgt_vocab_size=len(vocab_transform[TGT_LANGUAGE]),\n",
    "    dim_feedforward=512,\n",
    ").to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "summary(model)\n",
    "# for main_name, main_module in model.named_children():\n",
    "#     print(main_name)\n",
    "#     for sub_name, sub_module in main_module.named_children():\n",
    "#         print(\"└\", sub_name)\n",
    "#         for ssub_name, ssub_module in sub_module.named_children():\n",
    "#             print(\"│  └\", ssub_name)\n",
    "#             for sssub_name, sssub_module in ssub_module.named_children():\n",
    "#                 print(\"│  │  └\", sssub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(valid_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(source, target):\n",
      "['And they went out quickly, and fled from the sepulchre; for they trembled and were amazed: neither said they any thing to any man; for they were afraid.'\n",
      " '여자들이 심히 놀라 떨며 나와 무덤에서 도망하고 무서워하여 아무에게 아무 말도 하지 못하더라']\n",
      "source_batch: torch.Size([53, 128])\n",
      "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [  10,  183,   10,  ...,   10,   10,  542],\n",
      "        [  23,   60,  118,  ...,  349, 1208,  768],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1]])\n",
      "target_batch: torch.Size([47, 128])\n",
      "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [ 317,  559, 2592,  ...,    7, 1243,   51],\n",
      "        [  11,  142,    9,  ..., 1217,    4,  181],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 입력 텐서에 적용할 여러 전처리 함수를 연속적으로 적용하는 함수 정의\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# 입력 토큰 ID 시퀀스에 BOS와 EOS 토큰을 추가하는 함수 정의\n",
    "def input_transform(token_ids):\n",
    "    return torch.cat(\n",
    "        (torch.tensor([BOS_IDX]), torch.tensor(token_ids), torch.tensor([EOS_IDX]))\n",
    "    )\n",
    "\n",
    "# 배치 데이터를 정렬하기 위한 콜레이터(collator) 함수 정의\n",
    "def collator(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        # 각 샘플에 대해 토큰화 및 단어 집합 인덱싱 적용하여 소스 및 타겟 시퀀스 생성\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    # 패딩을 적용하여 소스 및 타겟 시퀀스를 배치 텐서로 변환\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# 각 언어에 대한 텍스트 전처리 함수를 저장하는 딕셔너리 초기화\n",
    "text_transform = {}\n",
    "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # 토큰화, 단어 집합 인덱싱 및 특수 토큰 추가 함수를 연속적으로 적용하여 정의\n",
    "    text_transform[language] = sequential_transforms(\n",
    "        token_transform[language], vocab_transform[language], input_transform\n",
    "    )\n",
    "\n",
    "# 데이터로더 생성\n",
    "dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collator) # val_iter가 데이터셋인듯\n",
    "# 다음 배치를 가져와서 소스 및 타겟 텐서 확인\n",
    "source_tensor, target_tensor = next(iter(dataloader)) \n",
    "\n",
    "print(\"(source, target):\")\n",
    "print(next(iter(val_iter)))\n",
    "\n",
    "print(\"source_batch:\", source_tensor.shape)\n",
    "print(source_tensor)\n",
    "\n",
    "print(\"target_batch:\", target_tensor.shape)\n",
    "print(target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_mask: torch.Size([53, 53])\n",
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]], device='cuda:0')\n",
      "target_mask: torch.Size([46, 46])\n",
      "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "source_padding_mask: torch.Size([128, 53])\n",
      "tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]])\n",
      "target_padding_mask: torch.Size([128, 46])\n",
      "tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "def generate_square_subsequent_mask(s):\n",
    "    mask = (torch.triu(torch.ones((s, s), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = (\n",
    "        mask.float()\n",
    "        .masked_fill(mask == 0, float(\"-inf\"))\n",
    "        .masked_fill(mask == 1, float(0.0))\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "\n",
    "target_input = target_tensor[:-1, :]\n",
    "target_out = target_tensor[1:, :]\n",
    "\n",
    "source_mask, target_mask, source_padding_mask, target_padding_mask = create_mask(\n",
    "    source_tensor, target_input\n",
    ")\n",
    "\n",
    "print(\"source_mask:\", source_mask.shape)\n",
    "print(source_mask)\n",
    "print(\"target_mask:\", target_mask.shape)\n",
    "print(target_mask)\n",
    "print(\"source_padding_mask:\", source_padding_mask.shape)\n",
    "print(source_padding_mask) # 사용 X\n",
    "print(\"target_padding_mask:\", target_padding_mask.shape)\n",
    "print(target_padding_mask) # 사용 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run(model, optimizer, criterion, split):\n",
    "    model.train() if split == \"train\" else model.eval()\n",
    "    data_iter = train_iter if split == 'train' else val_iter\n",
    "    # data_iter = Multi30k(split=split, language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    dataloader = DataLoader(data_iter, batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "    \n",
    "    losses = 0\n",
    "    pbar = tqdm(dataloader, total=len(list(dataloader)))\n",
    "    for source_batch, target_batch in pbar:\n",
    "        source_batch = source_batch.to(DEVICE)\n",
    "        target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "        target_input = target_batch[:-1, :]\n",
    "        target_output = target_batch[1:, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
    "            source_batch, target_input\n",
    "        )\n",
    "\n",
    "        logits = model(\n",
    "            src=source_batch,\n",
    "            trg=target_input,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_padding_mask=src_padding_mask,\n",
    "            tgt_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask,\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_output.reshape(-1))\n",
    "        if split == \"train\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_description(f\"loss: {loss:.6f}\")\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/195 [00:00<?, ?it/s]c:\\Users\\DELL-PC\\.conda\\envs\\EXAM_DL\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "c:\\Users\\DELL-PC\\.conda\\envs\\EXAM_DL\\lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "loss: 7.209983: 100%|██████████| 195/195 [01:10<00:00,  2.78it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 7.060, Val loss: 6.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 6.208398: 100%|██████████| 195/195 [00:52<00:00,  3.71it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 6.211, Val loss: 6.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 5.659228: 100%|██████████| 195/195 [00:52<00:00,  3.75it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 5.603, Val loss: 5.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 5.315501: 100%|██████████| 195/195 [00:52<00:00,  3.72it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 5.259, Val loss: 5.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 5.008731: 100%|██████████| 195/195 [00:52<00:00,  3.70it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 5.026, Val loss: 5.754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.763139: 100%|██████████| 195/195 [00:52<00:00,  3.69it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 4.852, Val loss: 5.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.576105: 100%|██████████| 195/195 [00:52<00:00,  3.71it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 4.719, Val loss: 5.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.420839: 100%|██████████| 195/195 [00:52<00:00,  3.70it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 4.603, Val loss: 5.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.201972: 100%|██████████| 195/195 [00:52<00:00,  3.71it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 4.500, Val loss: 5.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.067131: 100%|██████████| 195/195 [00:53<00:00,  3.66it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 4.415, Val loss: 5.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.918840: 100%|██████████| 195/195 [00:52<00:00,  3.70it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train loss: 4.343, Val loss: 5.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.819987: 100%|██████████| 195/195 [00:54<00:00,  3.61it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train loss: 4.279, Val loss: 5.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.699083: 100%|██████████| 195/195 [00:52<00:00,  3.70it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train loss: 4.221, Val loss: 5.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.568624: 100%|██████████| 195/195 [00:52<00:00,  3.70it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train loss: 4.161, Val loss: 5.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.464662: 100%|██████████| 195/195 [00:52<00:00,  3.70it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train loss: 4.101, Val loss: 5.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.356106: 100%|██████████| 195/195 [00:52<00:00,  3.70it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train loss: 4.043, Val loss: 5.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.307815: 100%|██████████| 195/195 [00:52<00:00,  3.68it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train loss: 3.990, Val loss: 5.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.219374: 100%|██████████| 195/195 [00:52<00:00,  3.72it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train loss: 3.940, Val loss: 5.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.119520: 100%|██████████| 195/195 [00:53<00:00,  3.62it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train loss: 3.908, Val loss: 5.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.056475: 100%|██████████| 195/195 [00:53<00:00,  3.66it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train loss: 3.870, Val loss: 5.822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.910067: 100%|██████████| 195/195 [00:53<00:00,  3.62it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train loss: 3.815, Val loss: 5.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.869355: 100%|██████████| 195/195 [00:53<00:00,  3.65it/s]\n",
      "100%|██████████| 49/49 [00:10<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train loss: 3.764, Val loss: 5.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.825975: 100%|██████████| 195/195 [00:53<00:00,  3.65it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train loss: 3.711, Val loss: 5.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.730878: 100%|██████████| 195/195 [00:53<00:00,  3.64it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train loss: 3.671, Val loss: 5.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.675660: 100%|██████████| 195/195 [00:53<00:00,  3.68it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train loss: 3.631, Val loss: 5.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.653659: 100%|██████████| 195/195 [00:52<00:00,  3.71it/s]\n",
      "100%|██████████| 49/49 [00:10<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train loss: 3.596, Val loss: 5.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.582213: 100%|██████████| 195/195 [00:52<00:00,  3.68it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train loss: 3.563, Val loss: 5.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.555850: 100%|██████████| 195/195 [00:53<00:00,  3.67it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train loss: 3.533, Val loss: 5.992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.496168: 100%|██████████| 195/195 [00:52<00:00,  3.68it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train loss: 3.501, Val loss: 6.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.448464: 100%|██████████| 195/195 [00:53<00:00,  3.64it/s]\n",
      "100%|██████████| 49/49 [00:10<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train loss: 3.469, Val loss: 6.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.414606: 100%|██████████| 195/195 [00:53<00:00,  3.64it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Train loss: 3.441, Val loss: 6.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.404557: 100%|██████████| 195/195 [00:53<00:00,  3.64it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Train loss: 3.415, Val loss: 6.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.375420: 100%|██████████| 195/195 [00:53<00:00,  3.63it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Train loss: 3.391, Val loss: 6.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.411699: 100%|██████████| 195/195 [00:53<00:00,  3.64it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Train loss: 3.369, Val loss: 6.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.312495: 100%|██████████| 195/195 [00:53<00:00,  3.65it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Train loss: 3.346, Val loss: 6.071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.261808: 100%|██████████| 195/195 [00:54<00:00,  3.61it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Train loss: 3.327, Val loss: 6.084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.304255: 100%|██████████| 195/195 [00:53<00:00,  3.61it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Train loss: 3.305, Val loss: 6.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.219018: 100%|██████████| 195/195 [00:54<00:00,  3.59it/s]\n",
      "100%|██████████| 49/49 [00:10<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Train loss: 3.285, Val loss: 6.121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.156316: 100%|██████████| 195/195 [00:53<00:00,  3.64it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Train loss: 3.264, Val loss: 6.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.160208: 100%|██████████| 195/195 [00:54<00:00,  3.60it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Train loss: 3.244, Val loss: 6.153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.138895: 100%|██████████| 195/195 [00:54<00:00,  3.60it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Train loss: 3.224, Val loss: 6.171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.102466: 100%|██████████| 195/195 [00:54<00:00,  3.61it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Train loss: 3.206, Val loss: 6.189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.096887: 100%|██████████| 195/195 [00:54<00:00,  3.60it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Train loss: 3.186, Val loss: 6.198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.107754: 100%|██████████| 195/195 [00:53<00:00,  3.63it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Train loss: 3.167, Val loss: 6.218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.073656: 100%|██████████| 195/195 [00:54<00:00,  3.59it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Train loss: 3.152, Val loss: 6.226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.076963: 100%|██████████| 195/195 [00:53<00:00,  3.62it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Train loss: 3.134, Val loss: 6.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.044893: 100%|██████████| 195/195 [00:53<00:00,  3.63it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Train loss: 3.118, Val loss: 6.237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.020242: 100%|██████████| 195/195 [00:54<00:00,  3.56it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Train loss: 3.103, Val loss: 6.251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.993401: 100%|██████████| 195/195 [00:53<00:00,  3.67it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Train loss: 3.089, Val loss: 6.271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.012260: 100%|██████████| 195/195 [00:53<00:00,  3.61it/s]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Train loss: 3.075, Val loss: 6.290\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    train_loss = run(model, optimizer, criterion, \"train\")\n",
    "    val_loss = run(model, optimizer, criterion, \"valid\")\n",
    "    print(f\"Epoch: {epoch+1}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'eng2kor2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# called_model = Seq2SeqTransformer(\n",
    "#     num_encoder_layers=6,\n",
    "#     num_decoder_layers=6,\n",
    "#     emb_size=8,\n",
    "#     max_len=128,\n",
    "#     nhead=8,\n",
    "#     src_vocab_size=len(vocab_transform[SRC_LANGUAGE]),\n",
    "#     tgt_vocab_size=len(vocab_transform[TGT_LANGUAGE]),\n",
    "#     dim_feedforward=512,\n",
    "# ).to(DEVICE)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "# checkpoint = torch.load('eng2kor.pth')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# # 모델을 평가 모드로 설정\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source_tensor, source_mask, max_len, start_symbol):\n",
    "    source_tensor = source_tensor.to(DEVICE)\n",
    "    source_mask = source_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(source_tensor, source_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len - 1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        target_mask = generate_square_subsequent_mask(ys.size(0))\n",
    "        target_mask = target_mask.type(torch.bool).to(DEVICE)\n",
    "\n",
    "        out = model.decode(ys, memory, target_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.ones(1, 1).type_as(source_tensor.data).fill_(next_word)], dim=0\n",
    "        )\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "def translate(model, source_sentence):\n",
    "    model.eval()\n",
    "    source_tensor = text_transform[SRC_LANGUAGE](source_sentence).view(-1, 1)\n",
    "    num_tokens = source_tensor.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model, source_tensor, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX\n",
    "    ).flatten()\n",
    "    output = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))[1:-1]\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예수 께서 제자 들 에게 이르시되 너희 가 떡 을 살것이 떡 을 가지사 축사 하시고\n",
      "예수 께서 제자 들 에게 이르시되 너희 가 어찌하여 성전 에 들어가기가 어려우니라\n"
     ]
    }
   ],
   "source": [
    "# 태초에 하나님이 천지를 창조하시니라\n",
    "output_oov = translate(model, \"In the beginning God created the pasta and the pizza.\")\n",
    "output = translate(model, \"In the beginning God created the heaven and the earth.\")\n",
    "print(output_oov)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
