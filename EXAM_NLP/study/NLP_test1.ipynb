{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:18:20.061974700Z",
     "start_time": "2024-03-30T07:18:19.287178600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary 단어 : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n",
      "문장 단어 빈도수 : ['You know I want your love. because I love you'] => [[1 1 2 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"You know I want your love. because I love you\"]\n",
    "vector = CountVectorizer() # 인스턴스 생성\n",
    "\n",
    "vector.fit(corpus) # 단어 사전 생성\n",
    "print(f\"vocabulary 단어 : {vector.vocabulary_}\")\n",
    "\n",
    "result = vector.transform(corpus).toarray() # 단어별 빈도수\n",
    "print(f\"문장 단어 빈도수 : {corpus} => {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab => ['과일이', '길고', '노란', '먹고', '바나나', '사과', '싶은', '저는', '좋아요']\n"
     ]
    },
    {
     "data": {
      "text/plain": "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n0    0   0   0   1    0   1   1   0    0\n1    0   0   0   1    1   0   1   0    0\n2    0   1   1   0    2   0   0   0    0\n3    1   0   0   0    0   0   0   1    1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>과일이</th>\n      <th>길고</th>\n      <th>노란</th>\n      <th>먹고</th>\n      <th>바나나</th>\n      <th>사과</th>\n      <th>싶은</th>\n      <th>저는</th>\n      <th>좋아요</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "docs = docs = [ '먹고 싶은 사과', '먹고 싶은 바나나', '길고 노란 바나나 바나나', '저는 과일이 좋아요'  ]\n",
    "\n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()\n",
    "\n",
    "print(f\"vocab => {vocab}\")\n",
    "\n",
    "# 총 문서의 수 \n",
    "N = len(docs)\n",
    "\n",
    "# 문서에 나타난 단어 빈도 수\n",
    "def tf(t, d):\n",
    " return d.count(t)\n",
    "\n",
    "# 총 문서에서 단어가 나타난 빈도\n",
    "def idf(t):\n",
    " df=0\n",
    " for doc in docs :   df += t in doc\n",
    " return log( N/(df+1) )\n",
    "\n",
    "# TF-IDF 계산 \n",
    "def tfidf(t, d):\n",
    " return tf(t,d) & idf(t)\n",
    "\n",
    "result =[]\n",
    "# 각 문서에 대해서 아래 연산 반복\n",
    "for i in range(N):\n",
    " result.append([])\n",
    " d = docs[i]\n",
    " for j in range(len(vocab)):\n",
    "  t = vocab[j]\n",
    "  result[-1].append(tf(t, d))\n",
    "     \n",
    "tf_ = pd.DataFrame(result, columns=vocab)\n",
    "tf_\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T08:15:48.178054300Z",
     "start_time": "2024-03-30T08:15:48.148837100Z"
    }
   },
   "id": "aea5744818fbdf62",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    " print(\"a\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T08:15:22.266094800Z",
     "start_time": "2024-03-30T08:15:22.252996700Z"
    }
   },
   "id": "5ec18d7ee0bac18e",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happy', ',', 'new', 'year', '!', 'Do', \"n't\", 'stop', '.']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Happy, new year! Don't stop.\"\n",
    "\n",
    "result = word_tokenize(text)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T08:54:34.094798Z",
     "start_time": "2024-03-30T08:54:33.312622600Z"
    }
   },
   "id": "c03c25c57725ab5a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happy', ',', 'new', 'year', '!', 'Don', \"'\", 't', 'stop', '.']\n"
     ]
    }
   ],
   "source": [
    "# 구두점 분리 토큰화\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "text = \"Happy, new year! Don't stop.\"\n",
    "\n",
    "wp_tokenizer = WordPunctTokenizer()\n",
    "result = wp_tokenizer.tokenize(text)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T08:58:28.810346Z",
     "start_time": "2024-03-30T08:58:28.800790700Z"
    }
   },
   "id": "be95a981ff256b22",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위 토큰화\n",
    "from nltk import sent_tokenize\n",
    "text = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "You can see it out your window or on your television. \\\n",
    "You feel it when you go to work, or go to church or pay your taxes.'\n",
    "result = sent_tokenize(text)\n",
    "print(result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T09:02:44.428289300Z",
     "start_time": "2024-03-30T09:02:44.417043600Z"
    }
   },
   "id": "b7dc88a5044587a6",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "# 길이가 1~2인 단어들 정규 표현식 이용하여 삭제\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T09:03:24.819858700Z",
     "start_time": "2024-03-30T09:03:24.781030400Z"
    }
   },
   "id": "87bc325f3ffebea5",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything'], 불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words_list = stopwords.words('english')\n",
    "\n",
    "print(\"불용어 개수 :\", len(stop_words_list))\n",
    "print(\"불용어 10개 출력 :\", stop_words_list[:10])\n",
    "\n",
    "# stop words 제거\n",
    "example = \"Family is not an important thing. It's everything\"\n",
    "stop_words = set(stopwords.words('english')) # 굳이 set을 왜?\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for word in word_tokens:\n",
    " if word not in stop_words_list:result.append(word)\n",
    " \n",
    "print(f\"불용어 제거 전 : {word_tokens}, 불용어 제거 후 : {result}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T09:14:28.950875700Z",
     "start_time": "2024-03-30T09:14:28.937023500Z"
    }
   },
   "id": "1db7954085111a42",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['policy', 'the going', 'am', 'doing', 'organization', 'having', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n"
     ]
    }
   ],
   "source": [
    "# 어간 추출\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "words =  [ 'policy', 'the going', 'am', 'doing', 'organization', 'having', 'going', 'love',\n",
    " 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print(f\"어간 추출 전 : {words}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T09:15:51.452524600Z",
     "start_time": "2024-03-30T09:15:51.428741200Z"
    }
   },
   "id": "83c523585a09a047",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['policy', 'the going', 'am', 'doing', 'organization', 'having', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후 : ['policy', 'the going', 'am', 'doing', 'organization', 'having', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words =  ['policy', 'the going', 'am', 'doing', 'organization', 'having', 'going', 'love', 'lives', 'fly', \n",
    "'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "result = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(f\"표제어 추출 전 : {words}\")\n",
    "print(f\"표제어 추출 후 : {result}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T09:31:42.973672100Z",
     "start_time": "2024-03-30T09:31:42.950860800Z"
    }
   },
   "id": "b3487430fd53d40",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"\n",
    "\n",
    "tokenizer1 = RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer2 = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "\n",
    "print(tokenizer1.tokenize(text))\n",
    "print(tokenizer2.tokenize(text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T09:33:19.644133800Z",
     "start_time": "2024-03-30T09:33:19.614164400Z"
    }
   },
   "id": "52169bc2ca6f80fc",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 자연어 전처리 적용"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d6af82a211cd7c7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T11:13:56.546765200Z",
     "start_time": "2024-03-30T11:13:56.523276500Z"
    }
   },
   "id": "a7f82b5963958c41",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                                             review\n0           0  Watching Time Chasers, it obvious that it was ...\n1           1  I saw this film about 20 years ago and remembe...\n2           2  Minor Spoilers In New York, Joan Barnard (Elvi...\n3           3  I went to see this film with a great deal of e...\n4           4  Yes, I agree with everyone on this site this m...\n5           5  Jennifer Ehle was sparkling in \\\"Pride and Pre...\n6           6  Amy Poehler is a terrific comedian on Saturday...\n7           7  A plane carrying employees of a large biotech ...\n8           8  A well made, gritty science fiction movie, it ...\n9           9  Incredibly dumb and utterly predictable story ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Watching Time Chasers, it obvious that it was ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>I saw this film about 20 years ago and remembe...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Minor Spoilers In New York, Joan Barnard (Elvi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>I went to see this film with a great deal of e...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Yes, I agree with everyone on this site this m...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>Jennifer Ehle was sparkling in \\\"Pride and Pre...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>Amy Poehler is a terrific comedian on Saturday...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>A plane carrying employees of a large biotech ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>A well made, gritty science fiction movie, it ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>Incredibly dumb and utterly predictable story ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"imdb.tsv\", delimiter=\"\\t\")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T11:16:19.134751200Z",
     "start_time": "2024-03-30T11:16:19.105787600Z"
    }
   },
   "id": "579984e15326faee",
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 대소문자 통합"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2ee7de2a57e7ced"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'watching time chasers, it obvious that it was made by a bunch of friends. maybe they were sitting around one day in film school and said, \\\\\"hey, let\\'s pool our money together and make a really bad movie!\\\\\" or something like that. what ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc. all corners were cut, except the one that would have prevented this film\\'s release. life\\'s like that.'"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"] = df[\"review\"].str.lower()\n",
    "df[\"review\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T11:16:20.390165800Z",
     "start_time": "2024-03-30T11:16:20.375257400Z"
    }
   },
   "id": "bed6f5aa1369fe62",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 단어 토큰화"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7081795eb6a5d3d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[\"word_tokens\"] = df[\"review\"].apply(word_tokenize) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T11:17:17.005480200Z",
     "start_time": "2024-03-30T11:17:16.979020200Z"
    }
   },
   "id": "76b5259cb215abc5",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['watching', 'time', 'chasers', ',', 'it', 'obvious', 'that', 'it', 'was', 'made', 'by', 'a', 'bunch', 'of', 'friends', '.', 'maybe', 'they', 'were', 'sitting', 'around', 'one', 'day', 'in', 'film', 'school', 'and', 'said', ',', '\\\\', \"''\", 'hey', ',', 'let', \"'s\", 'pool', 'our', 'money', 'together', 'and', 'make', 'a', 'really', 'bad', 'movie', '!', '\\\\', \"''\", 'or', 'something', 'like', 'that', '.', 'what', 'ever', 'they', 'said', ',', 'they', 'still', 'ended', 'up', 'making', 'a', 'really', 'bad', 'movie', '--', 'dull', 'story', ',', 'bad', 'script', ',', 'lame', 'acting', ',', 'poor', 'cinematography', ',', 'bottom', 'of', 'the', 'barrel', 'stock', 'music', ',', 'etc', '.', 'all', 'corners', 'were', 'cut', ',', 'except', 'the', 'one', 'that', 'would', 'have', 'prevented', 'this', 'film', \"'s\", 'release', '.', 'life', \"'s\", 'like', 'that', '.']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"word_tokens\"][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T11:17:26.394960600Z",
     "start_time": "2024-03-30T11:17:26.387983500Z"
    }
   },
   "id": "5f6fa1549f0be76e",
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터 정제"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de91f80e52b891a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from NLP_preprocess import clean_by_freq, clean_by_len, clean_by_stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T11:18:34.043616800Z",
     "start_time": "2024-03-30T11:18:33.977972500Z"
    }
   },
   "id": "944601a442cb78d6",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "list"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "type(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T11:20:03.784172700Z",
     "start_time": "2024-03-30T11:20:03.720232Z"
    }
   },
   "id": "365681805cb46e4",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[\"cleaned_tokens\"] = df[\"word_tokens\"].apply(lambda x : clean_by_freq(x, 1))\n",
    "df[\"cleaned_tokens\"] = df[\"cleaned_tokens\"].apply(lambda x :clean_by_len(x, 2))\n",
    "df[\"cleaned_tokens\"] = df[\"cleaned_tokens\"].apply(lambda x :clean_by_stopwords(x, stopwords_set))\n",
    "# apply는 적용할 함수 이름 하나만 넣을 수 있어가지고,,, 내가 만든 함수 쓰려면 lambda를 써야함 "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T11:22:35.260322100Z",
     "start_time": "2024-03-30T11:22:35.206638700Z"
    }
   },
   "id": "a2f9749f8b8bc5b3",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0    12\n1    12\n2    18\ndtype: int64"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_(x):\n",
    " return sum(x)\n",
    "\n",
    "test_df = pd.DataFrame([[1, 2, 3],[4, 5, 6],[7, 5, 9]])\n",
    "test_df.apply(sum_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T11:28:53.022697Z",
     "start_time": "2024-03-30T11:28:52.958344200Z"
    }
   },
   "id": "17a989097c6776e2",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aca014f1feb8fe50"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
