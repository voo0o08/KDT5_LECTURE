{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-02T05:38:18.380982Z",
     "start_time": "2024-04-02T05:38:17.835039600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\kdp\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\kdp\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus_df = pd.DataFrame(corpus.test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T05:39:03.765286Z",
     "start_time": "2024-04-02T05:39:02.724717200Z"
    }
   },
   "id": "caf4caca250037da",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainDF = corpus_df.sample(frac=0.9, random_state=42)\n",
    "testDF = corpus_df.drop(trainDF.index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T05:49:50.437282300Z",
     "start_time": "2024-04-02T05:49:50.413255400Z"
    }
   },
   "id": "aec995e682e2a1a3",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.to_markdown of                                                     text  label\n",
      "33553  모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만...      1\n",
      "9427                    무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...      0\n",
      "199                                          신날 것 없는 애니.      0\n",
      "12447                                              잔잔 격동      1\n",
      "39489                                 오랜만에 찾은 주말의 명화의 보석      1>\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "print(trainDF.head().to_markdown)\n",
    "print(\"Training Data Size :\", len(trainDF))\n",
    "print(\"Testing Data Size :\", len(testDF))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T05:49:50.591740700Z",
     "start_time": "2024-04-02T05:49:50.566563800Z"
    }
   },
   "id": "7a40bd857b8853d3",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 단어 사전 생성\n",
    "- 토큰화 진행 => 형태소 분석기 선택\n",
    "- 단어 사전"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bab93b771e493f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-1 토큰화 진행(문장 to 단어)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f1fcf71cae3101a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 토큰화 인스턴스 생성\n",
    "tokenizer = Okt()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T05:49:51.496938100Z",
     "start_time": "2024-04-02T05:49:51.477386600Z"
    }
   },
   "id": "60686714264f9304",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습은 데이터 크기가 작은 테스트 데이터세트를 활용해서 실습 진행"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae673a92fb358e49"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "33553    모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만...\n9427                      무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...\n199                                            신날 것 없는 애니.\n12447                                                잔잔 격동\n39489                                   오랜만에 찾은 주말의 명화의 보석\n42724         영화공사중 도로를 보았는데 에어컨을.. 정말 더웠다.하지만 더글라스오빠연기 압권\n10822                      재밌다. 유쾌한 노랫소리. 주인공들의 코믹한 연기. 굿굿\n49498                                  음.. 괜찮네요. 생각보다 좋았어요\n4144     \"\"\"21세기 \"\"\"\"레니 할린, 작품은 \"\"\"\"마인트헌터, 뿐인가!? 연출이 tv...\n36958    이걸 영화관에서 보다니 감동이었고 관객이 다들 혼자온 덕후들이라 더욱 감개무량했던 ...\nName: text, dtype: object"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 => 단어 분리\n",
    "trainDF.text[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T05:49:51.990800100Z",
     "start_time": "2024-04-02T05:49:51.978870500Z"
    }
   },
   "id": "2e03b181f11929a3",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 문장 => 단어 분리 \n",
    "# for text in trainDF.text:\n",
    "#     print(tokenizer.morphs(text, stem=True)) # stem = 어근만 \n",
    "#     break\n",
    "\n",
    "train_tokens = [tokenizer.morphs(text, stem=True) for text in trainDF.text]\n",
    "test_tokens = [tokenizer.morphs(text, stem=True) for text in testDF.text]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T05:53:59.180006600Z",
     "start_time": "2024-04-02T05:52:50.830820100Z"
    }
   },
   "id": "f81c8da378575483",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train_tokens] 45000개\n",
      "[test_tokens] 5000개\n",
      "[test_tokens 0번] 19개\n",
      "[test_tokens 0번] 18개\n",
      "[test_tokens 1번] 14개\n",
      "[test_tokens 1번] 6개\n"
     ]
    }
   ],
   "source": [
    "print(f\"[train_tokens] {len(train_tokens)}개\")\n",
    "print(f\"[test_tokens] {len(test_tokens)}개\")\n",
    "print(f\"[test_tokens 0번] {len(train_tokens[0])}개\")\n",
    "print(f\"[test_tokens 0번] {len(test_tokens[0])}개\")\n",
    "print(f\"[test_tokens 1번] {len(train_tokens[1])}개\")\n",
    "print(f\"[test_tokens 1번] {len(test_tokens[1])}개\")\n",
    "# 리스트 내의 요소의 개수가 제각각인 것을 알 수 있음 "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T05:56:47.964875300Z",
     "start_time": "2024-04-02T05:56:47.933953900Z"
    }
   },
   "id": "8e5ef354f0cbefed",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-2 토큰화 => 단어/어휘 사전 생성"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b8bf90ee5e876f0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 단어 사전 생성 함수\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    \n",
    "    # 단어/토큰에 대한 빈도수 계산\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "        \n",
    "    # 단어/어휘 사전 생성\n",
    "    vocab = special_tokens\n",
    "    \n",
    "    # 단어/어휘 사전에 빈도수가 높은 단어 추가\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:04:43.284388Z",
     "start_time": "2024-04-02T06:04:43.262020400Z"
    }
   },
   "id": "2c027eec02aab83c",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VOCAB => ['<PAD>', '<UNK>', '.', '이', '영화', '보다', '하다', '의', '..', '에', '가', '...', '을', '도', '들', ',', '는', '를', '은', '없다', '이다', '있다', '좋다', '?', '너무', '다', '정말', '한', '되다', '재밌다']]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "vocab = build_vocab(train_tokens, 5000, [\"<PAD>\", \"<UNK>\"])\n",
    "print(f\"[VOCAB => {vocab[:30]}]\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:08:35.823733400Z",
     "start_time": "2024-04-02T06:08:35.744987600Z"
    }
   },
   "id": "d8d475a9205c5de8",
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-3 인코딩 & 디코딩 인덱싱 만들기 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6ae15f3bc1981da"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<UNK>', '.', '이', '영화', '보다', '하다', '의', '..', '에']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "# 인코딩 : 문자 => 순자\n",
    "token_to_id = {v:id for id, v in enumerate(vocab)}\n",
    "\n",
    "# 디코딩 : 숫자 => 문자 \n",
    "id_to_token = {id:v for id, v in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:12:07.627688300Z",
     "start_time": "2024-04-02T06:12:07.616080500Z"
    }
   },
   "id": "f92474c33484db9a",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 리뷰의 문자를 정수로 변환 및 단어 사건 어휘 사전에 없는 문자로 처리\n",
    "UNK_ID = token_to_id.get(\"<UNK>\")\n",
    "train_ids = [[token_to_id.get(token, UNK_ID) for token in text] for text in train_tokens]\n",
    "test_ids = [[token_to_id.get(token, UNK_ID) for token in text] for text in test_tokens] "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:12:30.530508300Z",
     "start_time": "2024-04-02T07:12:30.442616300Z"
    }
   },
   "id": "a9fdff1079f66025",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ids => 45000개\n",
      "test_ids => 5000개\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_ids => {len(train_ids)}개\")\n",
    "print(f\"test_ids => {len(test_ids)}개\") # 5000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:12:31.465515Z",
     "start_time": "2024-04-02T07:12:31.443706600Z"
    }
   },
   "id": "85761b8b4ec9d9aa",
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. 데이터 가공 \n",
    "- 토큰 데이터 정수 인코딩\n",
    "- 데이터 길이 표준화 => 다른 길이의 데이터를 길어 맞추기(그래야 시퀀스가 통일됨) -> 1개 문장 구성하는 단어 맞추기 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e179b549fa5f0987"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3-1 토큰 정수화"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9feaf1c824728f1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3-2 데이터 구성 단어 수 맞추기 즉, 패딩\n",
    "- 단어 수 선정 필요\n",
    "- 선정된 단어 수에 맞게 데이터 길면 자르거, 짧으면 치워 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40daad44a0066276"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 명선생님 ver -> 왼쪽 default\n",
    "# def pad_sequences(sequences, max_length, pad_value, start=\"L\"):\n",
    "#     result = list()\n",
    "#     for sequence in sequences:\n",
    "#         sequence = sequence[:max_length] if start == \"L\" else sequence[-1*max_length:]\n",
    "#         pad_length = max_length - len(sequence)\n",
    "#         padded_sequence = sequence + [pad_value] * pad_length if start == \"L\" else  [pad_value] * pad_length + sequence\n",
    "#         result.append(padded_sequence)\n",
    "#     return np.asarray(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:12:33.261844300Z",
     "start_time": "2024-04-02T07:12:33.242253800Z"
    }
   },
   "id": "2cf4359d875d7d51",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 패딩 처리 함수\n",
    "# sentence : 토큰화된 문장 데이터\n",
    "# max_length : 최대 문장길이 즉, 1개 문장 구성 단어수\n",
    "# pad : 패딩 처리 시 추가될 문자 값\n",
    "# start : 패딩 시 처리 방향 [RL : 오른쪽 즉, 뒷부분 자르기/추가하기]\n",
    "def pad_sequences(sequences, max_len, pad, start=\"R\"):\n",
    "    result = []\n",
    "    for sen in sequences:\n",
    "        sen = sen[:max_len] if start == \"R\" else sen[:-1*max_len]\n",
    "        padd_sen = sen + [pad]*(max_len-len(sen)) if start == \"R\" else ([pad]*(max_len-len(sen))) + sen\n",
    "        result.append(padd_sen)\n",
    "    \n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:12:33.486143Z",
     "start_time": "2024-04-02T07:12:33.466172100Z"
    }
   },
   "id": "df1e5c9c2be64b20",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 학습용, 테스트용 데이터 패딩 처리\n",
    "PAD_ID = token_to_id.get(\"<PAD>\")\n",
    "MAX_LENGTH = 32\n",
    "\n",
    "train_ids = pad_sequences(train_ids, MAX_LENGTH, PAD_ID)\n",
    "test_ids = pad_sequences(test_ids, MAX_LENGTH, PAD_ID)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:12:33.822296700Z",
     "start_time": "2024-04-02T07:12:33.781834700Z"
    }
   },
   "id": "a2cebd6f797008c4",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_idx => 32 개, [258, 1621, 12, 1370, 171, 222, 365, 4, 2, 2101, 1044, 255, 36, 15, 3993, 1, 1, 1029, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "test_idx => 32 개\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_idx => {len(train_ids[0])} 개, {train_ids[0]}\")\n",
    "print(f\"test_idx => {len(test_ids[0])} 개\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:12:33.896906900Z",
     "start_time": "2024-04-02T07:12:33.877265100Z"
    }
   },
   "id": "ed1da8f0dba68224",
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. 데이터 학습 준비\n",
    "- 데이터 로더 준비\n",
    "- 학습용/테스트용 함수\n",
    "- 모델 클래스\n",
    "- 학습 관련 변수 => DEVICE, OPTIMIZER, MODEL 인스턴스, EPOCHS, BATCH_SIZE, LOSS_FN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b0e04215efdf46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4-1 데이터 로더 준비"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "926cfe2ae4a8aa02"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:17:41.086078800Z",
     "start_time": "2024-04-02T07:17:41.078695300Z"
    }
   },
   "id": "5b3a04943ce1c9ee",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataTS => torch.Size([45000, 32]), labelTS => torch.Size([45000])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 생성 : List => Tensor \n",
    "# train_ids => 토큰이 정수로 바뀐 상태 List\n",
    "# Label => trainDF.label.values\n",
    "\n",
    "dataTS = torch.LongTensor(train_ids)\n",
    "labelTS = torch.FloatTensor(trainDF.label.values)\n",
    "\n",
    "print(f\"dataTS => {dataTS.shape}, labelTS => {labelTS.shape}\")\n",
    "\n",
    "trainDS = TensorDataset(dataTS, labelTS) # train 데이터셋 생성"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:47:39.231108400Z",
     "start_time": "2024-04-02T07:47:39.213379600Z"
    }
   },
   "id": "f4cbb28920a748f8",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataTS => torch.Size([5000, 32]), labelTS => torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "# 학습용 데이터셋\n",
    "dataTS_ = torch.LongTensor(test_ids)\n",
    "labelTS_ = torch.FloatTensor(testDF.label.values)\n",
    "\n",
    "print(f\"dataTS => {dataTS_.shape}, labelTS => {labelTS_.shape}\")\n",
    "\n",
    "testDS = TensorDataset(dataTS_, labelTS_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:47:39.638274400Z",
     "start_time": "2024-04-02T07:47:39.601743800Z"
    }
   },
   "id": "983f8150657f4dfc",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 데이터로더 생성\n",
    "BATCH_SIZE = 32\n",
    "trainDL = DataLoader(trainDS, BATCH_SIZE, shuffle=True)\n",
    "testDL = DataLoader(testDS, BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:47:39.927787700Z",
     "start_time": "2024-04-02T07:47:39.906833700Z"
    }
   },
   "id": "9257b09be20fe906",
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4-2 모델 클래스 정의\n",
    "- 입력층 : Embedding Layer\n",
    "- 은닉층 : RNN/LSTM Layer\n",
    "- 은닉층 : dropout Layer\n",
    "- 출력층 : Linear Layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af197935fd6da868"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentenceClassfier(nn.Module):\n",
    "    def __init__(self, n_vocab, hidden_dim, embedding_dim, n_layers, dropout=0.5, bidirectional=True, model_type=\"lstm\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab, # 피쳐 개수\n",
    "            embedding_dim=embedding_dim, # 줄이고 나서 피쳐 개수(진짜 내 맘대로)\n",
    "            padding_idx=0\n",
    "        )\n",
    "        \n",
    "        if model_type == \"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            \n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        \n",
    "        # 양방향이면 방향성이 2이라서 2곱해주고\n",
    "        # 단방향이면 1이라서 1곱해줌(1잉께 생략 가능)\n",
    "        if bidirectional: \n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim * 1, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:47:40.252066800Z",
     "start_time": "2024-04-02T07:47:40.232951Z"
    }
   },
   "id": "d61480e5931f0250",
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "n_vocab = len(token_to_id) # 피쳐개수 = 단어 개수\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = SentenceClassfier(n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:47:40.375638700Z",
     "start_time": "2024-04-02T07:47:40.362124700Z"
    }
   },
   "id": "461c2147eacbbdd6",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.7076765298843384\n",
      "Train Loss 500 : 0.6832133019993643\n",
      "Train Loss 1000 : 0.6494185156100518\n",
      "Val Loss : 0.5431304589198653, Val Accuracy : 0.7412\n",
      "Train Loss 0 : 0.5376872420310974\n",
      "Train Loss 500 : 0.4924626992848105\n",
      "Train Loss 1000 : 0.4769163194980536\n",
      "Val Loss : 0.4179986618506681, Val Accuracy : 0.8118\n",
      "Train Loss 0 : 0.3111460208892822\n",
      "Train Loss 500 : 0.38709146759943097\n",
      "Train Loss 1000 : 0.3788888713011851\n",
      "Val Loss : 0.39128512922365954, Val Accuracy : 0.822\n",
      "Train Loss 0 : 0.5061667561531067\n",
      "Train Loss 500 : 0.33768229828980156\n",
      "Train Loss 1000 : 0.34008305528751026\n",
      "Val Loss : 0.3947492801839379, Val Accuracy : 0.8248\n",
      "Train Loss 0 : 0.24726185202598572\n",
      "Train Loss 500 : 0.3133950291458004\n",
      "Train Loss 1000 : 0.31034168120209393\n",
      "Val Loss : 0.3969756348687372, Val Accuracy : 0.8278\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, trainDL, criterion, optimizer, device, interval)\n",
    "    test(classifier, testDL, criterion, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:50:37.284587500Z",
     "start_time": "2024-04-02T07:47:40.571236100Z"
    }
   },
   "id": "d888defc11e82736",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:50:37.300717100Z",
     "start_time": "2024-04-02T07:50:37.286646300Z"
    }
   },
   "id": "241b3f534f982c91",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:50:37.317471900Z",
     "start_time": "2024-04-02T07:50:37.301714900Z"
    }
   },
   "id": "941c0970dc353b9d",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:50:37.333182700Z",
     "start_time": "2024-04-02T07:50:37.318469Z"
    }
   },
   "id": "41f24e559f2ef791",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "43669cdb3cdd70a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
