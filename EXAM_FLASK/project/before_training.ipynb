{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Your input_length: 31 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "c:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Your input_length: 91 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': '우리 주 예수 그리스도의 은혜가 너희와 함께 하십시요. 아멘. 주'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 90 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': '그러므로 하루 아침에 역병이 몰려와 죽고 애통함과 기근이 오고 불로'}]\n",
      "[{'translation_text': '그리고 그녀와 함께 살면서 처절한 시련을 겪고 맛있게 살았던 땅의'}]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"translation\", model=\"circulus/kobart-trans-ko-en-v2\")\n",
    "# pipe(\"오늘 점심으로 스테이크를 먹었습니다.\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"translation\", model=\"dylanmengzhou/kobart-trans-en-ko-v2\")\n",
    "print(pipe(\"The grace of our Lord Jesus Christ be with you all. Amen.\"))\n",
    "print(pipe(\"Therefore shall her plagues come in one day, death, and mourning, and famine; and she shall be utterly burned with fire: for strong is the Lord God who judgeth her.\"))\n",
    "print(pipe(\"And the kings of the earth, who have committed fornication and lived deliciously with her, shall bewail her, and lament for her, when they shall see the smoke of her burning,\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 테이블의 데이터수는 총 31102개 입니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pymysql\n",
    "\n",
    "# 데이터 베이스 연결하기\n",
    "conn = pymysql.connect(host='1.251.203.204',\n",
    "                       user='root',\n",
    "                       password='kdt5',\n",
    "                       db='Team4',\n",
    "                       charset='utf8',\n",
    "                       port=33065)\n",
    "\n",
    "curs = conn.cursor()\n",
    "\n",
    "# 검색 명령어 사용 \n",
    "sql = \"SELECT eng.text as eng_text, kor.text as kor_text FROM language_en eng join language_ko kor on eng.id = kor.id;\"\n",
    "curs.execute(sql)\n",
    "result = curs.fetchall()\n",
    "print(\"현재 테이블의 데이터수는 총 {}개 입니다.\".format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\AppData\\Local\\Temp\\ipykernel_24900\\1190723113.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_text</th>\n",
       "      <th>kor_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the beginning God created the heaven and th...</td>\n",
       "      <td>태초에 하나님이 천지를 창조하시니라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And the earth was without form, and void; and ...</td>\n",
       "      <td>땅이 혼돈하고 공허하며 흑암이 깊음 위에 있고 하나님의 신은 수면에 운행하시니라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>하나님이 가라사대 빛이 있으라 하시매 빛이 있었고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>그 빛이 하나님의 보시기에 좋았더라 하나님이 빛과 어두움을 나누사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>빛을 낮이라 칭하시고 어두움을 밤이라 칭하시니라 저녁이 되며 아침이 되니 이는 첫째...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            eng_text  \\\n",
       "0  In the beginning God created the heaven and th...   \n",
       "1  And the earth was without form, and void; and ...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God saw the light, that it was good: and G...   \n",
       "4  And God called the light Day, and the darkness...   \n",
       "\n",
       "                                            kor_text  \n",
       "0                                태초에 하나님이 천지를 창조하시니라  \n",
       "1       땅이 혼돈하고 공허하며 흑암이 깊음 위에 있고 하나님의 신은 수면에 운행하시니라  \n",
       "2                        하나님이 가라사대 빛이 있으라 하시매 빛이 있었고  \n",
       "3               그 빛이 하나님의 보시기에 좋았더라 하나님이 빛과 어두움을 나누사  \n",
       "4  빛을 낮이라 칭하시고 어두움을 밤이라 칭하시니라 저녁이 되며 아침이 되니 이는 첫째...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_sql(sql, conn)\n",
    "\n",
    "# 데이터베이스 연결 종료\n",
    "conn.close()\n",
    "\n",
    "# 쿼리 -> dataframe결과 출력\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_iterator(df):\n",
    "    for i, row in df.iterrows():\n",
    "        yield row[\"eng_text\"], row[\"kor_text\"] \n",
    "\n",
    "split_idx = int(len(df) * 0.8)  # 전체 데이터 중 80%를 훈련 데이터셋으로 사용\n",
    "train_df = df.iloc[:] # 걍 다 쓰세욤\n",
    "valid_df = df.iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\"id\":[],\n",
    "              \"translation\":[]}\n",
    "cnt = 0\n",
    "for value in train_df.values:\n",
    "    \n",
    "    train_dict[\"id\"].append(cnt)\n",
    "    \n",
    "    tempdict = {}\n",
    "    tempdict[\"eng\"] = value[0]\n",
    "    tempdict[\"kor\"] = value[1]\n",
    "    train_dict[\"translation\"].append(tempdict)\n",
    "    # print(value)\n",
    "    cnt += 1\n",
    "    # break\n",
    "\n",
    "train_df = pd.DataFrame(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = {\"id\":[],\n",
    "              \"translation\":[]}\n",
    "cnt = 0\n",
    "for value in valid_df.values:\n",
    "    \n",
    "    val_dict[\"id\"].append(cnt)\n",
    "    \n",
    "    tempdict = {}\n",
    "    tempdict[\"eng\"] = value[0]\n",
    "    tempdict[\"kor\"] = value[1]\n",
    "    val_dict[\"translation\"].append(tempdict)\n",
    "    # print(value)\n",
    "    cnt += 1\n",
    "    # break\n",
    "\n",
    "valid_df = pd.DataFrame(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 31102\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 6221\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "split_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(valid_df)  # 테스트 데이터도 동일한 데이터프레임을 사용하므로 train과 동일하게 처리\n",
    "})\n",
    "\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eng': 'And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.',\n",
       " 'kor': '땅이 혼돈하고 공허하며 흑암이 깊음 위에 있고 하나님의 신은 수면에 운행하시니라'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"dylanmengzhou/kobart-trans-en-ko-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [14603, 309, 299, 21235, 20041, 15562, 16160, 20676, 19542, 20676, 304, 16160, 17884, 315, 21579, 29709, 24536, 20290, 310, 18509, 258, 24536, 18785, 15562, 306, 309, 22873, 20676, 19542, 1700, 316, 311, 14889, 21235, 17254, 26605, 19524, 21235, 18785, 300, 29686, 245, 14603, 309, 299, 21235, 14470, 311, 21738, 16805, 19524, 15464, 20223, 19281, 310, 317, 17752, 1700, 316, 311, 14889, 21235, 17254, 26605, 19524, 21235, 20676, 16881, 27621, 245], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [15495, 12034, 14929, 9869, 14058, 14061, 13644, 14363, 16816, 11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466, 19808, 18940, 26418]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"eng\"]\n",
    "ko_sentence = split_datasets[\"train\"][1][\"translation\"][\"kor\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=ko_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁땅', '이', '▁혼', '돈', '하고', '▁공', '허', '하며', '▁흑', '암', '이', '▁깊', '음', '▁위에', '▁있고', '▁하나님의', '▁신', '은', '▁수', '면에', '▁운행', '하시', '니라']\n",
      "['▁땅', '이', '▁혼', '돈', '하고', '▁공', '허', '하며', '▁흑', '암', '이', '▁깊', '음', '▁위에', '▁있고', '▁하나님의', '▁신', '은', '▁수', '면에', '▁운행', '하시', '니라']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(ko_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"])) \n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128 \n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    inputs = [ex[\"eng\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"kor\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/31102 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 31102/31102 [00:02<00:00, 11340.37 examples/s]\n",
      "Map: 100%|██████████| 6221/6221 [00:00<00:00, 11416.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "\tpreprocessing_function, \n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15495, 12034, 14929,  9869, 14058, 14061, 13644, 14363, 16816, 11711,\n",
       "         12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466,\n",
       "         19808, 18940, 26418],\n",
       "        [14392, 16613, 22279, 11207,  9776, 27888, 17698, 10213, 20853, 10518,\n",
       "         27888, 22658,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 15495, 12034, 14929,  9869, 14058, 14061, 13644, 14363, 16816,\n",
       "         11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032,\n",
       "         18466, 19808, 18940],\n",
       "        [    1, 14392, 16613, 22279, 11207,  9776, 27888, 17698, 10213, 20853,\n",
       "         10518, 27888, 22658,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15495, 12034, 14929, 9869, 14058, 14061, 13644, 14363, 16816, 11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466, 19808, 18940, 26418]\n",
      "[14392, 16613, 22279, 11207, 9776, 27888, 17698, 10213, 20853, 10518, 27888, 22658]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\AppData\\Local\\Temp\\ipykernel_24900\\200089639.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "c:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [0, 0, 0, 0],\n",
       " 'totals': [11, 10, 9, 8],\n",
       " 'precisions': [0.0, 0.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 11,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"In the beginning God created the heaven and the earth.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "    \"태초에 하나님이 천지를 창조하시니라\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [0, 0, 0, 0],\n",
       " 'totals': [29, 28, 27, 26],\n",
       " 'precisions': [0.0, 0.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 29,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "    \"하나님이 궁창을 만드사 궁창 아래의 물과 궁창 위의 물로 나뉘게 하시매 그대로 되니라\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
