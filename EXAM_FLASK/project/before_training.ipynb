{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Your input_length: 31 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "c:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Your input_length: 91 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'ìš°ë¦¬ ì£¼ ì˜ˆìˆ˜ ê·¸ë¦¬ìŠ¤ë„ì˜ ì€í˜œê°€ ë„ˆí¬ì™€ í•¨ê»˜ í•˜ì‹­ì‹œìš”. ì•„ë©˜. ì£¼'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 90 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'ê·¸ëŸ¬ë¯€ë¡œ í•˜ë£¨ ì•„ì¹¨ì— ì—­ë³‘ì´ ëª°ë ¤ì™€ ì£½ê³  ì• í†µí•¨ê³¼ ê¸°ê·¼ì´ ì˜¤ê³  ë¶ˆë¡œ'}]\n",
      "[{'translation_text': 'ê·¸ë¦¬ê³  ê·¸ë…€ì™€ í•¨ê»˜ ì‚´ë©´ì„œ ì²˜ì ˆí•œ ì‹œë ¨ì„ ê²ªê³  ë§›ìˆê²Œ ì‚´ì•˜ë˜ ë•…ì˜'}]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"translation\", model=\"circulus/kobart-trans-ko-en-v2\")\n",
    "# pipe(\"ì˜¤ëŠ˜ ì ì‹¬ìœ¼ë¡œ ìŠ¤í…Œì´í¬ë¥¼ ë¨¹ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"translation\", model=\"dylanmengzhou/kobart-trans-en-ko-v2\")\n",
    "print(pipe(\"The grace of our Lord Jesus Christ be with you all. Amen.\"))\n",
    "print(pipe(\"Therefore shall her plagues come in one day, death, and mourning, and famine; and she shall be utterly burned with fire: for strong is the Lord God who judgeth her.\"))\n",
    "print(pipe(\"And the kings of the earth, who have committed fornication and lived deliciously with her, shall bewail her, and lament for her, when they shall see the smoke of her burning,\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ í…Œì´ë¸”ì˜ ë°ì´í„°ìˆ˜ëŠ” ì´ 31102ê°œ ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pymysql\n",
    "\n",
    "# ë°ì´í„° ë² ì´ìŠ¤ ì—°ê²°í•˜ê¸°\n",
    "conn = pymysql.connect(host='1.251.203.204',\n",
    "                       user='root',\n",
    "                       password='kdt5',\n",
    "                       db='Team4',\n",
    "                       charset='utf8',\n",
    "                       port=33065)\n",
    "\n",
    "curs = conn.cursor()\n",
    "\n",
    "# ê²€ìƒ‰ ëª…ë ¹ì–´ ì‚¬ìš© \n",
    "sql = \"SELECT eng.text as eng_text, kor.text as kor_text FROM language_en eng join language_ko kor on eng.id = kor.id;\"\n",
    "curs.execute(sql)\n",
    "result = curs.fetchall()\n",
    "print(\"í˜„ì¬ í…Œì´ë¸”ì˜ ë°ì´í„°ìˆ˜ëŠ” ì´ {}ê°œ ì…ë‹ˆë‹¤.\".format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\AppData\\Local\\Temp\\ipykernel_24900\\1190723113.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_text</th>\n",
       "      <th>kor_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the beginning God created the heaven and th...</td>\n",
       "      <td>íƒœì´ˆì— í•˜ë‚˜ë‹˜ì´ ì²œì§€ë¥¼ ì°½ì¡°í•˜ì‹œë‹ˆë¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And the earth was without form, and void; and ...</td>\n",
       "      <td>ë•…ì´ í˜¼ëˆí•˜ê³  ê³µí—ˆí•˜ë©° í‘ì•”ì´ ê¹ŠìŒ ìœ„ì— ìˆê³  í•˜ë‚˜ë‹˜ì˜ ì‹ ì€ ìˆ˜ë©´ì— ìš´í–‰í•˜ì‹œë‹ˆë¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>í•˜ë‚˜ë‹˜ì´ ê°€ë¼ì‚¬ëŒ€ ë¹›ì´ ìˆìœ¼ë¼ í•˜ì‹œë§¤ ë¹›ì´ ìˆì—ˆê³ </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>ê·¸ ë¹›ì´ í•˜ë‚˜ë‹˜ì˜ ë³´ì‹œê¸°ì— ì¢‹ì•˜ë”ë¼ í•˜ë‚˜ë‹˜ì´ ë¹›ê³¼ ì–´ë‘ì›€ì„ ë‚˜ëˆ„ì‚¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>ë¹›ì„ ë‚®ì´ë¼ ì¹­í•˜ì‹œê³  ì–´ë‘ì›€ì„ ë°¤ì´ë¼ ì¹­í•˜ì‹œë‹ˆë¼ ì €ë…ì´ ë˜ë©° ì•„ì¹¨ì´ ë˜ë‹ˆ ì´ëŠ” ì²«ì§¸...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            eng_text  \\\n",
       "0  In the beginning God created the heaven and th...   \n",
       "1  And the earth was without form, and void; and ...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God saw the light, that it was good: and G...   \n",
       "4  And God called the light Day, and the darkness...   \n",
       "\n",
       "                                            kor_text  \n",
       "0                                íƒœì´ˆì— í•˜ë‚˜ë‹˜ì´ ì²œì§€ë¥¼ ì°½ì¡°í•˜ì‹œë‹ˆë¼  \n",
       "1       ë•…ì´ í˜¼ëˆí•˜ê³  ê³µí—ˆí•˜ë©° í‘ì•”ì´ ê¹ŠìŒ ìœ„ì— ìˆê³  í•˜ë‚˜ë‹˜ì˜ ì‹ ì€ ìˆ˜ë©´ì— ìš´í–‰í•˜ì‹œë‹ˆë¼  \n",
       "2                        í•˜ë‚˜ë‹˜ì´ ê°€ë¼ì‚¬ëŒ€ ë¹›ì´ ìˆìœ¼ë¼ í•˜ì‹œë§¤ ë¹›ì´ ìˆì—ˆê³   \n",
       "3               ê·¸ ë¹›ì´ í•˜ë‚˜ë‹˜ì˜ ë³´ì‹œê¸°ì— ì¢‹ì•˜ë”ë¼ í•˜ë‚˜ë‹˜ì´ ë¹›ê³¼ ì–´ë‘ì›€ì„ ë‚˜ëˆ„ì‚¬  \n",
       "4  ë¹›ì„ ë‚®ì´ë¼ ì¹­í•˜ì‹œê³  ì–´ë‘ì›€ì„ ë°¤ì´ë¼ ì¹­í•˜ì‹œë‹ˆë¼ ì €ë…ì´ ë˜ë©° ì•„ì¹¨ì´ ë˜ë‹ˆ ì´ëŠ” ì²«ì§¸...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_sql(sql, conn)\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ\n",
    "conn.close()\n",
    "\n",
    "# ì¿¼ë¦¬ -> dataframeê²°ê³¼ ì¶œë ¥\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_iterator(df):\n",
    "    for i, row in df.iterrows():\n",
    "        yield row[\"eng_text\"], row[\"kor_text\"] \n",
    "\n",
    "split_idx = int(len(df) * 0.8)  # ì „ì²´ ë°ì´í„° ì¤‘ 80%ë¥¼ í›ˆë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©\n",
    "train_df = df.iloc[:] # ê± ë‹¤ ì“°ì„¸ìš¤\n",
    "valid_df = df.iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\"id\":[],\n",
    "              \"translation\":[]}\n",
    "cnt = 0\n",
    "for value in train_df.values:\n",
    "    \n",
    "    train_dict[\"id\"].append(cnt)\n",
    "    \n",
    "    tempdict = {}\n",
    "    tempdict[\"eng\"] = value[0]\n",
    "    tempdict[\"kor\"] = value[1]\n",
    "    train_dict[\"translation\"].append(tempdict)\n",
    "    # print(value)\n",
    "    cnt += 1\n",
    "    # break\n",
    "\n",
    "train_df = pd.DataFrame(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = {\"id\":[],\n",
    "              \"translation\":[]}\n",
    "cnt = 0\n",
    "for value in valid_df.values:\n",
    "    \n",
    "    val_dict[\"id\"].append(cnt)\n",
    "    \n",
    "    tempdict = {}\n",
    "    tempdict[\"eng\"] = value[0]\n",
    "    tempdict[\"kor\"] = value[1]\n",
    "    val_dict[\"translation\"].append(tempdict)\n",
    "    # print(value)\n",
    "    cnt += 1\n",
    "    # break\n",
    "\n",
    "valid_df = pd.DataFrame(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 31102\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 6221\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "split_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(valid_df)  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ë™ì¼í•œ ë°ì´í„°í”„ë ˆì„ì„ ì‚¬ìš©í•˜ë¯€ë¡œ trainê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬\n",
    "})\n",
    "\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eng': 'And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.',\n",
       " 'kor': 'ë•…ì´ í˜¼ëˆí•˜ê³  ê³µí—ˆí•˜ë©° í‘ì•”ì´ ê¹ŠìŒ ìœ„ì— ìˆê³  í•˜ë‚˜ë‹˜ì˜ ì‹ ì€ ìˆ˜ë©´ì— ìš´í–‰í•˜ì‹œë‹ˆë¼'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"dylanmengzhou/kobart-trans-en-ko-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [14603, 309, 299, 21235, 20041, 15562, 16160, 20676, 19542, 20676, 304, 16160, 17884, 315, 21579, 29709, 24536, 20290, 310, 18509, 258, 24536, 18785, 15562, 306, 309, 22873, 20676, 19542, 1700, 316, 311, 14889, 21235, 17254, 26605, 19524, 21235, 18785, 300, 29686, 245, 14603, 309, 299, 21235, 14470, 311, 21738, 16805, 19524, 15464, 20223, 19281, 310, 317, 17752, 1700, 316, 311, 14889, 21235, 17254, 26605, 19524, 21235, 20676, 16881, 27621, 245], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [15495, 12034, 14929, 9869, 14058, 14061, 13644, 14363, 16816, 11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466, 19808, 18940, 26418]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"eng\"]\n",
    "ko_sentence = split_datasets[\"train\"][1][\"translation\"][\"kor\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=ko_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–ë•…', 'ì´', 'â–í˜¼', 'ëˆ', 'í•˜ê³ ', 'â–ê³µ', 'í—ˆ', 'í•˜ë©°', 'â–í‘', 'ì•”', 'ì´', 'â–ê¹Š', 'ìŒ', 'â–ìœ„ì—', 'â–ìˆê³ ', 'â–í•˜ë‚˜ë‹˜ì˜', 'â–ì‹ ', 'ì€', 'â–ìˆ˜', 'ë©´ì—', 'â–ìš´í–‰', 'í•˜ì‹œ', 'ë‹ˆë¼']\n",
      "['â–ë•…', 'ì´', 'â–í˜¼', 'ëˆ', 'í•˜ê³ ', 'â–ê³µ', 'í—ˆ', 'í•˜ë©°', 'â–í‘', 'ì•”', 'ì´', 'â–ê¹Š', 'ìŒ', 'â–ìœ„ì—', 'â–ìˆê³ ', 'â–í•˜ë‚˜ë‹˜ì˜', 'â–ì‹ ', 'ì€', 'â–ìˆ˜', 'ë©´ì—', 'â–ìš´í–‰', 'í•˜ì‹œ', 'ë‹ˆë¼']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(ko_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"])) \n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128 \n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    inputs = [ex[\"eng\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"kor\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/31102 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31102/31102 [00:02<00:00, 11340.37 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6221/6221 [00:00<00:00, 11416.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "\tpreprocessing_function, \n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15495, 12034, 14929,  9869, 14058, 14061, 13644, 14363, 16816, 11711,\n",
       "         12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466,\n",
       "         19808, 18940, 26418],\n",
       "        [14392, 16613, 22279, 11207,  9776, 27888, 17698, 10213, 20853, 10518,\n",
       "         27888, 22658,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 15495, 12034, 14929,  9869, 14058, 14061, 13644, 14363, 16816,\n",
       "         11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032,\n",
       "         18466, 19808, 18940],\n",
       "        [    1, 14392, 16613, 22279, 11207,  9776, 27888, 17698, 10213, 20853,\n",
       "         10518, 27888, 22658,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15495, 12034, 14929, 9869, 14058, 14061, 13644, 14363, 16816, 11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466, 19808, 18940, 26418]\n",
      "[14392, 16613, 22279, 11207, 9776, 27888, 17698, 10213, 20853, 10518, 27888, 22658]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\AppData\\Local\\Temp\\ipykernel_24900\\200089639.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "c:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [0, 0, 0, 0],\n",
       " 'totals': [11, 10, 9, 8],\n",
       " 'precisions': [0.0, 0.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 11,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"In the beginning God created the heaven and the earth.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "    \"íƒœì´ˆì— í•˜ë‚˜ë‹˜ì´ ì²œì§€ë¥¼ ì°½ì¡°í•˜ì‹œë‹ˆë¼\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [0, 0, 0, 0],\n",
       " 'totals': [29, 28, 27, 26],\n",
       " 'precisions': [0.0, 0.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 29,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "    \"í•˜ë‚˜ë‹˜ì´ ê¶ì°½ì„ ë§Œë“œì‚¬ ê¶ì°½ ì•„ë˜ì˜ ë¬¼ê³¼ ê¶ì°½ ìœ„ì˜ ë¬¼ë¡œ ë‚˜ë‰˜ê²Œ í•˜ì‹œë§¤ ê·¸ëŒ€ë¡œ ë˜ë‹ˆë¼\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
