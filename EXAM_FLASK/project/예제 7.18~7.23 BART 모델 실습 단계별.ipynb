{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce5809b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate rouge_score absl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ae30fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "news = load_dataset(\"argilla/news-summary\", split=\"test\")\n",
    "df = news.to_pandas().sample(500, random_state=42)[[\"text\", \"prediction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c902e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prediction\"] = df[\"prediction\"].map(lambda x: x[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a1890f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 60%, 20%, 20% 로 데이터 분할\n",
    "train, valid, test = np.split(\n",
    "    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2997724f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>BUDAPEST (Reuters) - A Hungarian European Parl...</td>\n",
       "      <td>Hungary charges Jobbik MEP with spying on EU f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19455</th>\n",
       "      <td>WASHINGTON (Reuters) - The U.S. Air Force aske...</td>\n",
       "      <td>U.S. Air Force asks industry for proposals to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17237</th>\n",
       "      <td>(Reuters) - A third of Republican voters who s...</td>\n",
       "      <td>Exclusive: Blocking Trump could hurt Republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10842</th>\n",
       "      <td>BAGHDAD/ERBIL, Iraq (Reuters) - Opposition gro...</td>\n",
       "      <td>Opposition groups quit Iraqi Kurdish governmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18837</th>\n",
       "      <td>CLEVELAND/NEW YORK - Bracing for a general ele...</td>\n",
       "      <td>Democrats gird for fight with Trump in U.S. Ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12489</th>\n",
       "      <td>NEW YORK (Reuters) - Prospects that the presid...</td>\n",
       "      <td>U.S. options market not very 'Trumped up' ahea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4618</th>\n",
       "      <td>(Reuters) - Highlights for U.S. President Dona...</td>\n",
       "      <td>Highlights: The Trump presidency on April 13 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3062</th>\n",
       "      <td>WASHINGTON (Reuters) - U.S. lawmakers this wee...</td>\n",
       "      <td>A year later, U.S. lawmakers still take aim at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>BRIGHTON, England (Reuters) - Britain s opposi...</td>\n",
       "      <td>UK's Labour pledges infrastructure nationaliza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6444</th>\n",
       "      <td>KUALA LUMPUR (Reuters) - Malaysian police arre...</td>\n",
       "      <td>Three arrested in Malaysia for suspected beer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "4373   BUDAPEST (Reuters) - A Hungarian European Parl...   \n",
       "19455  WASHINGTON (Reuters) - The U.S. Air Force aske...   \n",
       "17237  (Reuters) - A third of Republican voters who s...   \n",
       "10842  BAGHDAD/ERBIL, Iraq (Reuters) - Opposition gro...   \n",
       "18837  CLEVELAND/NEW YORK - Bracing for a general ele...   \n",
       "...                                                  ...   \n",
       "12489  NEW YORK (Reuters) - Prospects that the presid...   \n",
       "4618   (Reuters) - Highlights for U.S. President Dona...   \n",
       "3062   WASHINGTON (Reuters) - U.S. lawmakers this wee...   \n",
       "1272   BRIGHTON, England (Reuters) - Britain s opposi...   \n",
       "6444   KUALA LUMPUR (Reuters) - Malaysian police arre...   \n",
       "\n",
       "                                              prediction  \n",
       "4373   Hungary charges Jobbik MEP with spying on EU f...  \n",
       "19455  U.S. Air Force asks industry for proposals to ...  \n",
       "17237  Exclusive: Blocking Trump could hurt Republica...  \n",
       "10842  Opposition groups quit Iraqi Kurdish governmen...  \n",
       "18837  Democrats gird for fight with Trump in U.S. Ru...  \n",
       "...                                                  ...  \n",
       "12489  U.S. options market not very 'Trumped up' ahea...  \n",
       "4618   Highlights: The Trump presidency on April 13 a...  \n",
       "3062   A year later, U.S. lawmakers still take aim at...  \n",
       "1272   UK's Labour pledges infrastructure nationaliza...  \n",
       "6444   Three arrested in Malaysia for suspected beer ...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0a583dd-df09-42a4-bed9-cbe3270fd368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source News : GAZA (Reuters) - A decade on, Rawda al-Zaanoun is at last willing to forgive the  gunmen who killed her son during the civil war that split Palestine. It has been painful, but she says it is time.  He\n",
      "Summarization : How blood money, diplomacy and desperation are reu\n",
      "Training Data Size : 300\n",
      "Validation Data Size : 100\n",
      "Testing Data Size : 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Source News : {train.text.iloc[0][:200]}\")          # 길이 200\n",
    "print(f\"Summarization : {train.prediction.iloc[0][:50]}\")   # 길이 50\n",
    "print(f\"Training Data Size : {len(train)}\")\n",
    "print(f\"Validation Data Size : {len(valid)}\")\n",
    "print(f\"Testing Data Size : {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "964b81ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "192b69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data, tokenizer, device):\n",
    "    tokenized = tokenizer(  # tokenized : transformers.tokenization_utils_base.BatchEncoding\n",
    "        text=data.text.tolist(),    # data 안의 text를 list로 변환\n",
    "        padding=\"longest\",          # 가장 긴 시퀀스의 길이에 맞춰 패딩을 추가\n",
    "        truncation=True,            # 입력 데이터가 모델의 최대입력길이를 초과하는 경우 자르는 작업\n",
    "        return_tensors=\"pt\",        # 토큰화된 결과를 pytorch tensor로 반환\n",
    "        max_length=256              # transformers 4.37.2 에서는 없어도 자동으로 길이고정됨. transformers 4.40.0 에서는 max_length를 줘야 고정됨\n",
    "    )   # tokenized : 'input_ids', 'attention_mask' 로 구성 (둘 다 tensor이고, shape : [3000, 3913])\n",
    "    labels = []\n",
    "    input_ids = tokenized[\"input_ids\"].to(device)\n",
    "    attention_mask = tokenized[\"attention_mask\"].to(device)\n",
    "    for target in data.prediction:\n",
    "        ### 요약 문자열을 토큰화, 정수화 하여 텐서로 저장\n",
    "        labels.append(tokenizer.encode(target, return_tensors=\"pt\").squeeze())\n",
    "    ### 입력으로 주어진 시퀀스 중 가장 긴 길이에 맞춰서 패딩을 수행한다. (shape : [30])\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100).to(device)\n",
    "    return TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "def get_datalodader(dataset, sampler, batch_size):\n",
    "    data_sampler = sampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53dcaf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 에폭, 배치사이즈, 디바이스, 토크나이저\n",
    "epochs = 3\n",
    "batch_size = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = BartTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"facebook/bart-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f77698e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bart.tokenization_bart.BartTokenizer"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61a66d8a-bb12-4b74-9ea7-529192ee562e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([    0,  4164, 22447,    36,  1251,    43,   111,    83,  2202,    15,\n",
      "            6,  8214,  6106,  1076,    12,  1301, 23081,  7928,    16,    23,\n",
      "           94,  2882,     7, 20184,     5,  1437, 18282,    54,   848,    69,\n",
      "          979,   148,     5,  2366,   997,    14,  3462, 16398,     4,    85,\n",
      "           34,    57,  8661,     6,    53,    79,   161,    24,    16,    86,\n",
      "            4,  1437,    91,    21,   478,    19,    10,  8894,    11,     5,\n",
      "          124,     4,    91,    21,    10, 26301,     6,  1437,     5,  4431,\n",
      "           12,   180,    12,   279,    26,    23,    41,   515,    11,  7914,\n",
      "          343,     7,  2458,     5,   285, 13229,     9,  1232,     9,    82,\n",
      "          848,    11,     5,   997,     4,  1437,    20,   568,    21,    45,\n",
      "         1365,   142,     5,  1925,     9,    84,   979,    16,  9761,     4,\n",
      "          125,    52,    33,   576, 23306,     4,  1405,   979, 18226,     6,\n",
      "           10,  2997,  1150,     9,    80,     8,    41,  1036,    11,     5,\n",
      "         5791,  4305,   573,  1572,     6,    21,   848,    11,   502,  3010,\n",
      "           71,    37,  6022,    66,     9,    39,   790,    11,  7914,   412,\n",
      "            6,   519,  1317,    14,    39, 11992,    21,  1710,    11, 10165,\n",
      "          227,  3429,  5791, 17954, 11547,     8, 11289,   895,     4,  1773,\n",
      "           14,   997,    10,  2202,   536,     6, 11289,   895,     6,   669,\n",
      "           30,     5, 16413, 36218,     9,   854,  2401,   853, 26560, 19987,\n",
      "            6,    34,   422,     5,   580,   788,     6,  3475,     5,  9275,\n",
      "         4984,  5791,  4305,     8,    57,  2149,    13,    70,  3377,    19,\n",
      "         1870,     4,  3139,  4346,     6,     5,  9954,   333, 11547,     6,\n",
      "           41,   160, 26286,     9,     5,  3346, 19767,     6,  4024, 11289,\n",
      "          895,    66,     9,  7914,     8,    34,   422,     5,  5262,  8095,\n",
      "         9572,    14,    16,   184,     7,   132,   153,    82,     6,   823,\n",
      "          457,     9,     5,  1956,     9,     2]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([    0,  6179,  1925,   418,     6, 15166,     8, 24278,    32, 10854,\n",
      "         2838, 16398,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100]))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = make_dataset(train, tokenizer, device)\n",
    "train_dataloader = get_datalodader(train_dataset, RandomSampler, batch_size)\n",
    "\n",
    "valid_dataset = make_dataset(valid, tokenizer, device)\n",
    "valid_dataloader = get_datalodader(valid_dataset, SequentialSampler, batch_size)\n",
    "\n",
    "test_dataset = make_dataset(test, tokenizer, device)\n",
    "test_dataloader = get_datalodader(test_dataset, SequentialSampler, batch_size)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3594bb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "x, y, z = train_dataset[0]\n",
    "print(x.shape)  \n",
    "print(y.shape)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df62dfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "024297d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "x, y, z = valid_dataset[0]\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7b85788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([26])\n"
     ]
    }
   ],
   "source": [
    "x, y, z = test_dataset[0]\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98b5a022-ff45-4824-846e-5b1193862b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"facebook/bart-base\"\n",
    ").to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b1e482e-5eb8-416f-a228-56788c328440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "└ shared\n",
      "└ encoder\n",
      "│  └ embed_tokens\n",
      "│  └ embed_positions\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ layernorm_embedding\n",
      "└ decoder\n",
      "│  └ embed_tokens\n",
      "│  └ embed_positions\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ layernorm_embedding\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│  └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                print(\"│  │  └\", sssub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e94d88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "import rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d142af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge(preds, labels):\n",
    "    preds = preds.argmax(axis=-1)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge2 = rouge_score.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )\n",
    "    return rouge2[\"rouge2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df0604e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for input_ids, attention_mask, labels in dataloader:\n",
    "        # print(f'input_ids => {input_ids.shape}')\n",
    "        # print(f'attention_mask => {attention_mask.shape}')\n",
    "        # print(f'labels => {labels.shape}')\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "827cd63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss, val_rouge = 0.0, 0.0\n",
    "\n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to(\"cpu\").numpy()\n",
    "            rouge = calc_rouge(logits, label_ids)\n",
    "            \n",
    "            val_loss += loss\n",
    "            val_rouge += rouge\n",
    "\n",
    "    val_loss = val_loss / len(dataloader)\n",
    "    val_rouge = val_rouge / len(dataloader)\n",
    "    return val_loss, val_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc1c0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4ad80cc-c0c2-4b03-9ab1-8fa50898ca62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.6907 Val Loss: 1.7039 Val Rouge 0.2861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [04:49<09:38, 289.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [10:17<05:12, 312.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.2185 Val Loss: 1.8984 Val Rouge 0.2534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [13:54:49<6:57:24, 25044.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluation(model, valid_dataloader)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Val Rouge \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[53], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, dataloader)\u001b[0m\n\u001b[0;32m     11\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rouge_score = evaluate.load(\"rouge\", tokenizer=tokenizer)\n",
    "best_loss = 10000\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = train(model, optimizer, train_dataloader)\n",
    "    val_loss, val_accuracy = evaluation(model, valid_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Val Rouge {val_accuracy:.4f}\")\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"BartForConditionalGeneration.pt\")\n",
    "        print(\"Saved the model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba9f3e-f939-4e77-b533-512c84921577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BartForConditionalGeneration.from_pretrained(\n",
    "#     pretrained_model_name_or_path=\"facebook/bart-base\"\n",
    "# ).to(device)\n",
    "# model.load_state_dict(torch.load(\"../models/BartForConditionalGeneration.pt\"))\n",
    "\n",
    "test_loss, test_rouge_score = evaluation(model, test_dataloader)\n",
    "print(f\"Test Loss : {test_loss:.4f}\")\n",
    "print(f\"Test ROUGE-2 Score : {test_rouge_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52711ca-faaa-489b-8f2d-4cf0780a2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "summarizer = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=54,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "for index in range(5):\n",
    "    news_text = test.text.iloc[index]\n",
    "    summarization = test.prediction.iloc[index]\n",
    "    predicted_summarization = summarizer(news_text)[0][\"summary_text\"]\n",
    "    print(f\"정답 요약문 : {summarization}\")\n",
    "    print(f\"모델 요약문 : {predicted_summarization}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99dace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a389f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c7326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
